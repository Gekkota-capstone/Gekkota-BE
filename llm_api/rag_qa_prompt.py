import os
import json
import faiss
import numpy as np
import openai
import time
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer
from sqlalchemy.orm import Session
from typing import List, Dict

from repository.chat_repository import ChatRepository
from repository.pet_repository import PetRepository
from repository.pet_health_repository import PetHealthRepository

CONVERSATION_LOG = "conversation_log.json"

# ===== ì„¤ì • =====
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FAISS_INDEX = os.path.join(BASE_DIR, "rag_faiss.index")
METADATA_JSON = os.path.join(BASE_DIR, "rag_metadata.json")
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
TOP_K = 3
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORG_ID = os.getenv("OPENAI_ORG_ID")
GPT_MODEL = "gpt-3.5-turbo-0125"
ALLOWED_KEYWORDS = [
    # ê¸°ë³¸ ì£¼ì œ
    "ë„ë§ˆë±€", "ê²Œì½”", "ë ˆì˜¤íŒŒë“œê²Œì½”", "ë ˆì˜¤íŒŒë“œ", "íŒŒì¶©ë¥˜", "ìƒë¬¼",

    # í–‰ë™ ë° ìŠ¤íŠ¸ë ˆìŠ¤
    "ìŠ¤íŠ¸ë ˆìŠ¤", "ì›€ì§ì„", "ì›€ì§ì´ì§€", "ê°€ë§Œíˆ", "ë©", "ë°˜ì‘ ì—†ìŒ", "ê³µê²©", "ë¬¼ì—ˆ", "ìœ„í˜‘", "ë„ë§", "ìˆ¨ì—ˆ", "í–‰ë™ ë³€í™”",

    # ë¨¹ì´ ê´€ë ¨
    "ë¨¹ì´", "ì‹ìš•", "ë°¥", "ì•ˆ ë¨¹", "ì˜ ë¨¹", "ê¸‰ì—¬", "ì‚¬ë£Œ", "ê°•ì œê¸‰ì—¬", "ì½”ì˜¤ë¡œê¸°", "ë°€ì›œ", "ì†Œí™”", "ì²´ì¤‘",

    # í™˜ê²½/ì‚¬ìœ¡ì¥
    "ì˜¨ë„", "ìŠµë„", "ì¡°ëª…", "UVB", "ë°”ë‹¥ì¬", "íˆí„°", "ë§¤íŠ¸", "ì¼€ì´ì§€", "ì‚¬ìœ¡ì¥", "í™˜ê²½", "ì€ì‹ ì²˜", "ë°°ê²½ì§€", "ì²­ì†Œ", "ì„¤ì¹˜", "í™˜ê¸°",

    # ê±´ê°•/ì§ˆë³‘/ìƒíƒœ
    "íƒˆí”¼", "ê¸°ìƒì¶©", "ìƒì²˜", "ëˆˆ", "í”¼ë¶€", "ê¼¬ë¦¬", "ë¶€ëŸ¬ì§", "ì§ˆë³‘", "ê°ì—¼", "ê±´ê°•", "ë¹„ì •ìƒ", "ì¦ìƒ", "ì°½ë°±", "ìƒ‰ì´ ë³€í•¨", "í˜ˆë³€", "êµ¬í† ", "ì•½",

    # ë°°ë³€
    "ë˜¥", "ë°°ë³€", "ë³€", "ì„¤ì‚¬", "ì†Œë³€", "ëŒ€ë³€", "ë°°ì„¤", "í™”ì¥ì‹¤", "ë³€ë¹„",

    # ì„±ì¥/ë‚˜ì´/ìˆ˜ëª…
    "ì„±ì¥", "í¬ê¸°", "ë‚˜ì´", "ëª‡ ì‚´", "ìˆ˜ëª…", "ì–¸ì œ ì„±ì²´", "ì„±ì²´", "ë…¸ë ¹", "ë…¸í™”",

    # ê´€ë¦¬ ë° ì²­ê²°
    "ì²­ì†Œ", "ìœ„ìƒ", "ì£¼ê¸°", "ë¬¼", "ì •ìˆ˜", "ë¨¹ì´í†µ", "ì˜¨ë„ê³„", "ìŠµë„ê³„",

    # ë¹„ìš© ë° ë³‘ì›
    "ë³‘ì›", "ì§„ë£Œ", "ì§„ë£Œë¹„", "ê°€ê²©", "ë¹„ìš©", "ì¹˜ë£Œ", "ìˆ˜ì˜ì‚¬", "ì•½ê°’", "ì§„ë‹¨", "ê²€ì‚¬", "ì˜ˆì•½",

    # í•©ì‚¬/ì‚¬ì´
    "í•©ì‚¬", "ì‹¸ì›€", "ë¬´ëŠ”", "ê°™ì´ í‚¤ì›Œ", "ìˆ˜ì»·", "ì•”ì»·", "ë²ˆì‹", "êµë°°", "ì„ì‹ ", "ì•Œ",

    # ìˆ˜ë©´/ì¼ìƒ
    "ì ", "ìëŠ”", "ë‚®ì ", "ë°¤ì— ì›€ì§", "ì•¼í–‰ì„±", "ë‚®ì—ë„ ì›€ì§", "í™œë™ ì‹œê°„",

    # ê¸°íƒ€ ì‹¤ìƒí™œ ì§ˆë¬¸ ìœ ì¶”
    "ì¶”ì²œ", "ì´ˆë³´", "ì²˜ìŒ", "í‚¤ìš°ê¸° ì‰¬ìš´", "ì²˜ìŒ í‚¤ì›€", "ì¥ë¹„", "ì¥ì ", "ë‹¨ì ", "ì£¼ì˜ì‚¬í•­", "ê¸°ì´ˆ", "ì²˜ë°©", "ìƒë‹´", "ì†Œë¦¬", "ìš¸ìŒì†Œë¦¬",

    # ëŒ€í™” íë¦„ / ë§ ê±¸ê¸° / ì¼ë°˜ í‘œí˜„ í—ˆìš©
    "ì•ˆë…•", "í•˜ì´", "ã…ã…‡", "ìˆì–´?", "ìˆë‚˜ìš”", "ìˆìŒ?", "ìˆë‹ˆ", "ì–´ë•Œ", "ê·¸ë˜ì„œ", "ì‘", "ë§ì•„", "ê·¸ëŸ¼",
    "ê·¼ë°", "í˜¹ì‹œ", "ì´ê±´", "ê·¸ê±´", "ì´ê²Œ", "ì €ê¸°", "ì ê¹", "ê³„ì†", "ë­ë”ë¼", "ê·¸ê±°", "ì´ë ‡ê²Œ", "ì €ë ‡ê²Œ",
    "ë‹¤ì‹œ", "ê³„ì†í•´", "ì´ì–´ì„œ", "ì¶”ê°€ë¡œ", "ë§í•´ì¤˜", "ì¢€ ë”", "ì¡°ê¸ˆ ë”", "ì •ë¦¬í•´ì¤˜", "ë„ì™€ì¤˜", "ì„¤ëª…í•´ì¤˜",
    "ë­ì•¼", "ë­”ë°", "ì™œ", "ì–´ë–»ê²Œ", "ì•Œë ¤ì¤˜", "ë§í•´", "ë‹µí•´ì¤˜", "ë­”ê°€", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ°ë°", "ë˜", "ê³„ì†", "ê·¸ëŸ¬ë©´"

]
SIMILARITY_THRESHOLD = 1.2
LOG_RETENTION_HOURS = 24

# ===== ì´ˆê¸°í™” =====
embed_model = None
index = None
metadata = None


def init_llm_system():
    global embed_model, index, metadata
    print("ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    openai.api_key = OPENAI_API_KEY
    openai.organization = OPENAI_ORG_ID
    embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    index = faiss.read_index(FAISS_INDEX)
    with open(METADATA_JSON, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")


def embed_text(text, model):
    return model.encode([text], convert_to_numpy=True)


def retrieve(query_embedding, index, metadata, top_k=TOP_K):
    distances, indices = index.search(query_embedding, top_k)
    results = []
    for idx in indices[0]:
        if idx < len(metadata):
            results.append(metadata[idx])
    return results, distances[0]


def truncate_text(text, max_chars):
    if not text:
        return ""
    return text if len(text) <= max_chars else text[:max_chars] + "..."


def get_pet_info(db: Session, pet_id: str, firebase_uid: str) -> dict:
    """Get pet information from database"""
    pet_repo = PetRepository()
    pet = pet_repo.get_by_id(db, pet_id)

    if not pet or pet.firebase_uid != firebase_uid:
        return None

    return {
        "id": pet.pet_id,
        "name": pet.name,
        "species": pet.species,
        "gender": pet.gender,
        "birthdate": pet.birthdate.isoformat() if pet.birthdate else None,
        "traits": []  # This would come from another table if needed
    }


def get_latest_health_record(db: Session, pet_id: str) -> dict:
    """Get the latest health record for a pet"""
    health_repo = PetHealthRepository()
    health_records = health_repo.get_by_pet(db, pet_id)

    if not health_records:
        return {
            "date": None,
            "weight": None,
            "memo": None,
            "shedding_status": None,
            "photo_urls": None
        }

    # Sort by date (most recent first)
    health_records.sort(key=lambda x: x.date, reverse=True)
    latest = health_records[0]

    return {
        "date": latest.date.isoformat() if latest.date else None,
        "weight": latest.weight,
        "memo": latest.memo,
        "shedding_status": latest.shedding_status,
        "photo_urls": None  # Assuming no photo URLs in current schema
    }


def format_pet_context(pet_info, latest_record):
    if not pet_info:
        return "[ë°˜ë ¤ë™ë¬¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤]"

    return f"""
[ë„ë§ˆë±€ í”„ë¡œí•„]
ì´ë¦„: {pet_info['name']}
ì¢…: {pet_info['species']}
ì„±ë³„: {pet_info['gender']}
ìƒì¼: {pet_info['birthdate']}
íŠ¹ì§•: {', '.join(pet_info['traits'])}

[ìµœê·¼ ê±´ê°• ê¸°ë¡]
ë‚ ì§œ: {latest_record['date']}
ëª¸ë¬´ê²Œ: {latest_record['weight']}g
ë©”ëª¨: {truncate_text(latest_record['memo'], 100)}
íƒˆí”¼ ìƒíƒœ: {latest_record['shedding_status']}
"""


def load_recent_conversation_from_db(db: Session, firebase_uid: str) -> List[Dict]:
    repo = ChatRepository()
    all_chats = repo.get_by_user(db, firebase_uid)

    # timezone-aware datetimeìœ¼ë¡œ ìƒì„±
    from datetime import timezone
    cutoff = datetime.now(timezone.utc) - timedelta(hours=LOG_RETENTION_HOURS)

    recent_chats = [
        chat for chat in all_chats
        if chat.created_at and chat.created_at > cutoff
    ]

    # Sort by created_at
    recent_chats.sort(key=lambda x: x.created_at)

    # Convert to conversation format
    conversation = []
    for chat in recent_chats:
        conversation.append({"role": "user", "question": chat.question})
        if chat.answer:
            conversation.append({"role": "assistant", "answer": chat.answer})

    return conversation


def is_valid_query(query, db: Session = None, firebase_uid: str = None):
    if not query.strip():
        return False  # ë¹ˆ ì…ë ¥ ë°©ì§€

    # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ì‚¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if any(k.replace(" ", "") in query.replace(" ", "") for k in ALLOWED_KEYWORDS):
        return True

    # DBì—ì„œ ìµœê·¼ ëŒ€í™” ê¸°ë¡ í™•ì¸
    if db and firebase_uid:
        try:
            history = load_recent_conversation_from_db(db, firebase_uid)
        except Exception as e:
            print(f"DB ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
            history = []
    else:
        history = []

    last_user_questions = [entry["question"] for entry in reversed(history) if entry["role"] == "user"][:5]

    # GPTì—ê²Œ íë¦„ì„ íŒë‹¨ì‹œí‚¤ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    context = "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìµœê·¼ ëŒ€í™” íë¦„ì…ë‹ˆë‹¤.\n"
    for i, q in enumerate(reversed(last_user_questions)):
        context += f"{i + 1}. {q}\n"
    context += f"{len(last_user_questions) + 1}. {query}\n\n"
    context += (
        "ì´ ëŒ€í™” íë¦„ì€ ì‚¬ìš©ìê°€ ë„ë§ˆë±€(ë˜ëŠ” íŒŒì¶©ë¥˜)ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ì´ì–´ê°€ëŠ” ì¼ê´€ëœ ì£¼ì œì˜ ëŒ€í™”ì…ë‹ˆê¹Œ?\n"
        "ë„ë§ˆë±€ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì§ˆë¬¸ë¿ ì•„ë‹ˆë¼, í•´ë‹¹ ì£¼ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê°ì • í‘œí˜„, ì™¸ëª¨ ì–¸ê¸‰, ë°˜ì‘ ìœ ë„, ìƒê° ê³µìœ , ì‚¬ìœ¡ ê´€ë ¨ ê°„ì ‘ì  ëŒ€í™” ë“±ë„ í¬í•¨ë©ë‹ˆë‹¤.\n"
        "ì˜ˆë¥¼ ë“¤ì–´ ì¸ì‚¬, ê°íƒ„, ì˜ê²¬ êµí™˜, ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²° ì§ˆë¬¸ ë“±ì€ ë„ë§ˆë±€ê³¼ì˜ ê´€ë ¨ì„±ì´ ì´ì–´ì§„ë‹¤ë©´ Yì…ë‹ˆë‹¤.\n"
        "ë‹¨, ë„ë§ˆë±€ê³¼ ë¬´ê´€í•œ ì¼ìƒ ëŒ€í™”(ì˜ˆ: ë‚ ì”¨, ìŒì‹, ë‰´ìŠ¤ ë“±)ë¡œ ì£¼ì œê°€ ëª…í™•íˆ ë°”ë€ ê²½ìš°ë§Œ Nìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.\n"
        "ëŒ€í™” íë¦„ì´ ë„ë§ˆë±€ ì¤‘ì‹¬ì´ë¼ë©´ ë°˜ë“œì‹œ Y, ê´€ë ¨ ì—†ë‹¤ë©´ Nìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”."
    )

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ëŒ€í™” íë¦„ì´ ì£¼ì œ ì•ˆì—ì„œ ìœ ì§€ë˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì•¼. ë°˜ë“œì‹œ Y ë˜ëŠ” Nìœ¼ë¡œë§Œ ë‹µí•´."},
                {"role": "user", "content": context}
            ],
            max_tokens=1,
            temperature=0
        )
        result = response.choices[0].message.content.strip().upper()
        print(f"ğŸ” íë¦„ ê¸°ë°˜ GPT íŒë‹¨: {result}")
        return result == "Y"
    except Exception as e:
        print("íë¦„ íŒë‹¨ ì˜¤ë¥˜:", e)
        return True  # ì˜¤ë¥˜ ì‹œ í†µê³¼


def generate_answer(contexts, question, pet_info=None, latest_record=None, db=None, firebase_uid=None):
    context_text = "\n".join([truncate_text(c["content"], 400) for c in contexts])
    personal_info = format_pet_context(pet_info, latest_record) if pet_info and latest_record else ""

    history = load_recent_conversation_from_db(db, firebase_uid) if db and firebase_uid else []

    message_history = [
                          {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ë§ˆë±€ ì‚¬ìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                      ] + [
                          {"role": "user", "content": h["question"]} if h.get("role") == "user" else {
                              "role": "assistant", "content": h["answer"]}
                          for h in history[-10:]
                      ]

    message_history.append({"role": "user", "content": f"""
{personal_info}

[ì§ˆë¬¸]
{question}

[ì°¸ê³ ìë£Œ]
{context_text}

ì§€ì¹¨:
1. ìœ„ì˜ ë°˜ë ¤ë™ë¬¼ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
2. ìµœê·¼ ê±´ê°• ê¸°ë¡ì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ í•˜ì„¸ìš”.
3. ì§ˆë¬¸ì´ ë„ë§ˆë±€ ê´€ë ¨ì´ ì•„ë‹ ê²½ìš°, "ì €ëŠ” ë„ë§ˆë±€ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.
4. ì‘ë‹µì€ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ë„ë¡ í•˜ì„¸ìš”.
"""})

    try:
        response = openai.ChatCompletion.create(
            model=GPT_MODEL,
            messages=message_history,
            temperature=0.3,
            max_tokens=800,  # 400 -> 800ìœ¼ë¡œ ì¦ê°€
            top_p=0.9  # ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."


def handle_query(query: str, pet_id: str, firebase_uid: str, db: Session) -> str:
    """
    Main function to handle user queries

    Args:
        query: User's question
        pet_id: Pet ID
        firebase_uid: User's Firebase UID
        db: Database session

    Returns:
        Generated answer
    """
    if not embed_model or not index or not metadata:
        raise RuntimeError("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. init_llm_system()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

    # Validate query
    if not is_valid_query(query, db, firebase_uid):
        return "â— ì´ ì‹œìŠ¤í…œì€ ë„ë§ˆë±€ ê´€ë ¨ ì§ˆë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."

    # Get pet information
    pet_info = get_pet_info(db, pet_id, firebase_uid)
    if not pet_info:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë°˜ë ¤ë™ë¬¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."

    # Get latest health record
    latest_record = get_latest_health_record(db, pet_id)

    # Retrieve relevant contexts
    query_emb = embed_text(query, embed_model)
    contexts, distances = retrieve(query_emb, index, metadata)
    if distances[0] > SIMILARITY_THRESHOLD:
        contexts = [{"content": ""}]

    # Generate answer
    return generate_answer(contexts, query, pet_info, latest_record, db, firebase_uid)